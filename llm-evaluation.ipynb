{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3c5f220e58fe4b00a5c90e586354446e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99e28040004e415eb84e11c66dff21ef","IPY_MODEL_b50f220d81994420b19171c01dad16b8","IPY_MODEL_7bd31bada0444e5682d74030848810d5"],"layout":"IPY_MODEL_54a8f20375774e81a46856d4aee0c66d"}},"99e28040004e415eb84e11c66dff21ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41624439c34349d8977918f83420a3f2","placeholder":"​","style":"IPY_MODEL_f8bbed179aad45148286cd3a2711bc71","value":"tokenizer_config.json: 100%"}},"b50f220d81994420b19171c01dad16b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58cde573cba84c0ea0daaf27269699bb","max":51052,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae7ad17ff7074c5b92f88277a5ffbcb0","value":51052}},"7bd31bada0444e5682d74030848810d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f53a1ed397ff4396a7fa6005939a6d37","placeholder":"​","style":"IPY_MODEL_07c7fd60b9a94a23a5ecfd54a79e6466","value":" 51.1k/51.1k [00:00&lt;00:00, 2.27MB/s]"}},"54a8f20375774e81a46856d4aee0c66d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41624439c34349d8977918f83420a3f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8bbed179aad45148286cd3a2711bc71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58cde573cba84c0ea0daaf27269699bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae7ad17ff7074c5b92f88277a5ffbcb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f53a1ed397ff4396a7fa6005939a6d37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07c7fd60b9a94a23a5ecfd54a79e6466":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2a63fd5853c4181980c732cbc11383c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d8fa7e8a94a45e8bdddc6e5cf12f727","IPY_MODEL_b33f5a89a0014c9aa21b0ec8b46cb086","IPY_MODEL_493ee4eb054a489da8b5212fed975166"],"layout":"IPY_MODEL_f890cbda2fd2435482dafae31f755386"}},"1d8fa7e8a94a45e8bdddc6e5cf12f727":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8b6a73329a84664bb466a2aa7a19e11","placeholder":"​","style":"IPY_MODEL_1d7959f1f1684c06bda0bc89aff58686","value":"tokenizer.json: 100%"}},"b33f5a89a0014c9aa21b0ec8b46cb086":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c62420fe9264c02bb306a0940e4802d","max":9085698,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd92f47eed3541d390648c313a8971d3","value":9085698}},"493ee4eb054a489da8b5212fed975166":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa4dca566974f1bad3852266a1d91f5","placeholder":"​","style":"IPY_MODEL_77342d45f6314681b03f37f6193ca879","value":" 9.09M/9.09M [00:09&lt;00:00, 998kB/s]"}},"f890cbda2fd2435482dafae31f755386":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8b6a73329a84664bb466a2aa7a19e11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d7959f1f1684c06bda0bc89aff58686":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c62420fe9264c02bb306a0940e4802d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd92f47eed3541d390648c313a8971d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fa4dca566974f1bad3852266a1d91f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77342d45f6314681b03f37f6193ca879":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17a0c1d5b60e48ecb974b98e5d23692c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8058bb20e2794b2c88ecb4c15570ddad","IPY_MODEL_c652876466964fb3ab0a7e95df5ee910","IPY_MODEL_4a54b102d5744ee582ddc15fbc90c554"],"layout":"IPY_MODEL_cf06731b0eee4e2e8d63b13feea27059"}},"8058bb20e2794b2c88ecb4c15570ddad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6edf610039d49b1922b2a729635ebbd","placeholder":"​","style":"IPY_MODEL_f6047ac3f0614a7c981bdc387774370e","value":"special_tokens_map.json: 100%"}},"c652876466964fb3ab0a7e95df5ee910":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd4759aeff3b49038a1a3f2d58a390ff","max":345,"min":0,"orientation":"horizontal","style":"IPY_MODEL_105d95654a1a4b41946f44ace79b9555","value":345}},"4a54b102d5744ee582ddc15fbc90c554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc01a1e58db94a098c0bd11d425b90fe","placeholder":"​","style":"IPY_MODEL_00c69d05d9594c74a0394dac19bdb4fd","value":" 345/345 [00:00&lt;00:00, 23.0kB/s]"}},"cf06731b0eee4e2e8d63b13feea27059":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6edf610039d49b1922b2a729635ebbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6047ac3f0614a7c981bdc387774370e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd4759aeff3b49038a1a3f2d58a390ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"105d95654a1a4b41946f44ace79b9555":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc01a1e58db94a098c0bd11d425b90fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00c69d05d9594c74a0394dac19bdb4fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10199472,"sourceType":"datasetVersion","datasetId":6302498},{"sourceId":10245905,"sourceType":"datasetVersion","datasetId":6336717}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install peft","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgZvLgYC9C2D","outputId":"bcf6dde3-7755-4c4f-a054-dd981d26bbbc","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:51:21.745225Z","iopub.execute_input":"2024-12-19T12:51:21.745593Z","iopub.status.idle":"2024-12-19T12:51:39.933447Z","shell.execute_reply.started":"2024-12-19T12:51:21.745560Z","shell.execute_reply":"2024-12-19T12:51:39.932315Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes\n!pip install torch","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9jcFixX_qKq","outputId":"eba3239f-c859-4f72-c56f-113c50992317","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:51:39.935372Z","iopub.execute_input":"2024-12-19T12:51:39.935713Z","iopub.status.idle":"2024-12-19T12:51:58.708488Z","shell.execute_reply.started":"2024-12-19T12:51:39.935684Z","shell.execute_reply":"2024-12-19T12:51:58.707325Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def generate_response(model, tokenizer, input_text, isTTM = False):\n    max_length = 512\n    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n    if not isTTM:\n      outputs = model.generate(input_ids['input_ids'], max_new_tokens=2048,temperature = 0.6, do_sample = True, top_k = 50,top_p = 0.95)\n    else:\n      outputs = model.generate(input_ids['input_ids'], max_new_tokens=max_length, num_beams = 2, early_stopping = True)\n    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return res","metadata":{"id":"MA2sLab89b0I","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:08:22.715495Z","iopub.execute_input":"2024-12-19T13:08:22.716481Z","iopub.status.idle":"2024-12-19T13:08:22.722654Z","shell.execute_reply.started":"2024-12-19T13:08:22.716406Z","shell.execute_reply":"2024-12-19T13:08:22.721475Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def generate_output(text,max_cot = 1):\n  buffer = []\n  buffer.append(text)\n  for i in range(max_cot):\n    input_text = f\"\"\"\n    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n    Role: You are an Instructions Providing AI that generates tailored steps to solve the given question.\n\n    Instructions:\n    - Carefully analyze the provided question before generating steps.\n    - Only generate **specific steps** that are relevant to solving this particular question.\n    - Avoid using generic or repetitive steps such as \"Identify key information\" or \"Verify the solution\".\n    - Focus on logical reasoning, calculations, or operations that are **directly necessary** to solve the question.\n    - Give Exactly 4 steps.\n    - Do not provide descriptions or explanations for the steps.\n    - Only output the **step titles** relevant to the question at hand.\n    - Follow the provided format:\n      Step 1: [Tailored Step Title According to Question]\n      Step 2: [Tailored Step Title According to Question]\n      Step 3: [Tailored Step Title According to Question]\n      Step 4: [Tailored Step Title According to Question]\n    - Do not generate an answer to the question or hint at the solution.\n    - Do not exceed 4 Steps\n    Generate the steps based solely on the question below.\n    <|eot_id|><|start_header_id|>user<|end_header_id|>\"{text}\"<|eot_id|>\"\"\"\n    internal_thought = generate_response(TTM_model, tokenizer, input_text,isTTM = True)\n    internal_thought = extract_final_answer(internal_thought)\n    internal_thought = f\"Internal_thought{i+1}:\"+\"\\n\"+internal_thought\n    buffer.append(internal_thought)\n    prompt = '\\n'.join(buffer)\n    final_input = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n    Role: You are a highly intelligent AI assistant specializing in mathematics and coding. Your expertise includes solving complex math problems, writing and debugging code, explaining mathematical concepts, and providing optimized solutions for coding challenges. When presented with a question or problem, you will: 1. Analyze the problem carefully. 2. Provide clear and concise explanations for your reasoning. 3. Offer step-by-step solutions for math and coding problems. 4. Generate clean, efficient, and well-commented code for programming tasks. You are expected to be accurate, logical, and detailed in your responses.\n    Instruction:\n    - Use the internal_thought to guide yourself to a correct answer and verify that it is correct before responding to the user.\n    - Final output needs to be an answer for the question.\n    - The last sentence needs to be the correct option for the question.\n    - Provide the index of the correct option\n    - Always provide the correct option number at the end\n    - Follow the strictly the Structure of output:\n        Explanation : Elaborate on steps in internal_thought provided\n        Answer : Correct Answer\n        Option : Correct Option number for the correct answer in the choices\n    - Do not deviate from the format mentioned above\n    - Option can only be any one value in 0,1,2,3 and should only be the option number\n    - Do not hallucinate\n    - Do not deviate from the instructions\n    Example:\n      Question : What is 1 + 2 ?\n      Choices:\n      0) 3\n      1) 1\n      2) 2\n      3) 4\n      Explanation : 1 + 2 adds to 3\n      Answer : the answer is 3\n      Option : 0\n    <|end_header_id|>\n    <|start_header_id|>user<|end_header_id|>\"{prompt}\"<|eot_id|>\"\"\"\n    final_output = generate_response(model_finetuned, tokenizer, final_input)\n    final_output = f\"{i+1}th Output:\\n\" + final_output\n    buffer.append(final_output)\n  return final_output","metadata":{"id":"VZje8dck9MX_","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:28:56.138925Z","iopub.execute_input":"2024-12-19T13:28:56.140165Z","iopub.status.idle":"2024-12-19T13:28:56.150069Z","shell.execute_reply.started":"2024-12-19T13:28:56.140114Z","shell.execute_reply":"2024-12-19T13:28:56.148940Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"!pip install -U peft","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rf1DCLj-BWb4","outputId":"ebd2113d-91b8-49b2-c83c-84358b644fca","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:51:58.735195Z","iopub.execute_input":"2024-12-19T12:51:58.735544Z","iopub.status.idle":"2024-12-19T12:52:07.251465Z","shell.execute_reply.started":"2024-12-19T12:51:58.735506Z","shell.execute_reply":"2024-12-19T12:52:07.250591Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_id = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_finetuned = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True,torch_dtype = torch.bfloat16).to(torch_device)\nmodel_finetuned.load_adapter('/kaggle/input/main-adapter')\nTTM_id = 'unsloth/Llama-3.2-1B-Instruct-bnb-4bit'\nTTM_model = AutoModelForCausalLM.from_pretrained(TTM_id,load_in_4bit=True,torch_dtype = torch.bfloat16).to(torch_device)\nTTM_model.load_adapter('/kaggle/input/adapter')\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281,"referenced_widgets":["3c5f220e58fe4b00a5c90e586354446e","99e28040004e415eb84e11c66dff21ef","b50f220d81994420b19171c01dad16b8","7bd31bada0444e5682d74030848810d5","54a8f20375774e81a46856d4aee0c66d","41624439c34349d8977918f83420a3f2","f8bbed179aad45148286cd3a2711bc71","58cde573cba84c0ea0daaf27269699bb","ae7ad17ff7074c5b92f88277a5ffbcb0","f53a1ed397ff4396a7fa6005939a6d37","07c7fd60b9a94a23a5ecfd54a79e6466","c2a63fd5853c4181980c732cbc11383c","1d8fa7e8a94a45e8bdddc6e5cf12f727","b33f5a89a0014c9aa21b0ec8b46cb086","493ee4eb054a489da8b5212fed975166","f890cbda2fd2435482dafae31f755386","a8b6a73329a84664bb466a2aa7a19e11","1d7959f1f1684c06bda0bc89aff58686","9c62420fe9264c02bb306a0940e4802d","bd92f47eed3541d390648c313a8971d3","4fa4dca566974f1bad3852266a1d91f5","77342d45f6314681b03f37f6193ca879","17a0c1d5b60e48ecb974b98e5d23692c","8058bb20e2794b2c88ecb4c15570ddad","c652876466964fb3ab0a7e95df5ee910","4a54b102d5744ee582ddc15fbc90c554","cf06731b0eee4e2e8d63b13feea27059","e6edf610039d49b1922b2a729635ebbd","f6047ac3f0614a7c981bdc387774370e","cd4759aeff3b49038a1a3f2d58a390ff","105d95654a1a4b41946f44ace79b9555","cc01a1e58db94a098c0bd11d425b90fe","00c69d05d9594c74a0394dac19bdb4fd"]},"id":"5aLAajjb_BiW","outputId":"8e6f01a5-a0a2-442e-e1e7-b082c90d8aca","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:53:19.922281Z","iopub.execute_input":"2024-12-19T12:53:19.922867Z","iopub.status.idle":"2024-12-19T12:56:23.232576Z","shell.execute_reply.started":"2024-12-19T12:53:19.922829Z","shell.execute_reply":"2024-12-19T12:56:23.231861Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1596def124554f27b7418b2d856ee55c"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351c67542eb84fdbbd249d2809dd26b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69dcae9094824a49a35e255b85b9cda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19af18eab33040cea3d7dc0d3a5575e2"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c0c9098ed64a96babaacbeb368f82c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc2c1c7dd66434cb9a4ae308dad11ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4fddb96e6b49c98e800515240799be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1b90c17e8e648a59bbe8e212d2d021d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf652cba12c8409c9e8e5405727071af"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\ncolumns = [\n    \"Model Name\",         # Name of the model\n    \"Dataset\",            # Dataset used (e.g., MMLU, GSM8K)\n    \"Task\",               # Specific task or subset of the dataset\n    \"Accuracy\",           # Accuracy metric\n    \"BLEU Score\",         # BLEU score metric\n    \"ROUGE-L Score\",      # ROUGE-L score metric\n]\n\n# Create an empty DataFrame with the specified columns\nevaluation_results = pd.DataFrame(columns=columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:57:58.217933Z","iopub.execute_input":"2024-12-19T12:57:58.218669Z","iopub.status.idle":"2024-12-19T12:57:58.228242Z","shell.execute_reply.started":"2024-12-19T12:57:58.218633Z","shell.execute_reply":"2024-12-19T12:57:58.227227Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"models = [\"unsloth/llama-3.1-8b-Instruct-bnb-4bit\",\"mistralai/Mistral-7B\",\"gemma2-9b\", \"qwen/Qwen-2-7B\"]\n#temp_model = AutoModelForCausalLM.from_pretrained(models[0],load_in_4bit=True,torch_dtype = torch.bfloat16).to(torch_device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:57:58.230070Z","iopub.execute_input":"2024-12-19T12:57:58.230314Z","iopub.status.idle":"2024-12-19T12:57:58.245210Z","shell.execute_reply.started":"2024-12-19T12:57:58.230289Z","shell.execute_reply":"2024-12-19T12:57:58.244325Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install datasets\n!pip install rouge-score\n!pip install nltk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CEyl8RkNwgR","outputId":"bad8d1af-1bb1-411f-8f5d-69cdea296407","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:57:58.246248Z","iopub.execute_input":"2024-12-19T12:57:58.246550Z","iopub.status.idle":"2024-12-19T12:58:24.916942Z","shell.execute_reply.started":"2024-12-19T12:57:58.246514Z","shell.execute_reply":"2024-12-19T12:58:24.915903Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=37b438bd53c77c90c305bea821b0c7f786ab749b4fa7fd6186d5044cf9c6cbe2\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"TTM_model.eval()\nmodel_finetuned.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGtbzDcyCh7s","outputId":"5378b537-526f-4886-a1ba-3a584e5312bf","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:58:45.735283Z","iopub.execute_input":"2024-12-19T12:58:45.735682Z","iopub.status.idle":"2024-12-19T12:58:45.768527Z","shell.execute_reply.started":"2024-12-19T12:58:45.735645Z","shell.execute_reply":"2024-12-19T12:58:45.767719Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Identity()\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=14336, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def extract_final_answer(output: str) -> str:\n    # Assuming \"Assistant:\" precedes the answer\n    if \"assistant\" in output:\n        temp = output.split(\"assistant\")[-1].strip()\n        res = temp.replace('assistant','')\n        return res\n    return output.strip()","metadata":{"id":"4Cn4ovKREih1","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:58:45.769764Z","iopub.execute_input":"2024-12-19T12:58:45.770011Z","iopub.status.idle":"2024-12-19T12:58:45.774268Z","shell.execute_reply.started":"2024-12-19T12:58:45.769986Z","shell.execute_reply":"2024-12-19T12:58:45.773509Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"2kWWpFXKIHly","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:58:45.775351Z","iopub.execute_input":"2024-12-19T12:58:45.775734Z","iopub.status.idle":"2024-12-19T12:58:45.788173Z","shell.execute_reply.started":"2024-12-19T12:58:45.775694Z","shell.execute_reply":"2024-12-19T12:58:45.787477Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\ndef evaluate_mmlu(name):\n    print(\"Evaluating on MMLU...\")\n\n    # Load MMLU dataset (example subset: 'high_school_mathematics')\n    dataset = load_dataset(\"cais/mmlu\", name, split=\"test\")\n\n    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    bleu_scores = []\n    rouge_scores = []\n    correct = 0\n    total = 0\n\n    for sample in tqdm(dataset, desc=\"Processing MMLU\", total=len(dataset)):\n        question = sample['question']\n        choices = sample['choices']  # List of options (A, B, C, D)\n        answer = sample['answer']  # Correct choice index\n        ground_truth = choices[answer]\n\n        # Create the input to the LLM (formatted question)\n        temp = '\\n'\n        for n,c in enumerate(choices):\n          temp += f'{n}) {c}\\n'\n        input_text = f\"Question: {question}\\nChoices: {temp}\"\n\n        # Generate response\n        output = generate_output(input_text).strip()\n        number = extract_answer_number(output)\n        rouge_score = rouge.score(ground_truth, output)['rougeL'].fmeasure\n        bleu_score = sentence_bleu([ground_truth.split()], output.split())\n        for i in range(len(output),len(ground_truth)):\n          if i + len(ground_truth) <= len(output):\n            rouge_score = max(rouge_score,rouge.score(ground_truth, output[i:i + len(ground_truth)])['rougeL'].fmeasure)\n            bleu_score = max(rouge_score,sentence_bleu([ground_truth.split()], output[i:i + len(ground_truth)].split()))\n        rouge_scores.append(rouge_score)\n        bleu_scores.append(bleu_score)\n\n        # Match output with options\n        print(f'answer:{answer} generated:{number}')\n        if answer == number:\n          correct += 1\n\n        total += 1\n\n    accuracy = correct / total\n    average_bleu = sum(bleu_scores) / total\n    average_rouge = sum(rouge_scores) / total\n    print(f\"MMLU Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    print(f\"MMLU Average BLEU Score: {average_bleu:.4f}\")\n    print(f\"MMLU Average ROUGE-L Score: {average_rouge:.4f}\")\n    return accuracy, average_bleu, average_rouge\n\ndef evaluate_gsm8k():\n    print(\"Evaluating on GSM8K...\")\n\n    # Load GSM8K dataset\n    dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n\n    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    bleu_scores = []\n    rouge_scores = []\n    correct = 0\n    total = 0\n\n    for sample in tqdm(dataset, desc=\"Processing GSM8K\", total=len(dataset)):\n        question = sample['question']\n        ground_truth = sample['answer']  # Ground truth answer as string\n\n        # Generate response\n        input_text = f\"Question: {question}\\nAnswer:\"\n        output = generate_output(input_text).strip()\n\n        # Calculate ROUGE and BLEU\n        rouge_score = rouge.score(ground_truth, output)['rougeL'].fmeasure\n        bleu_score = sentence_bleu([ground_truth.split()], output.split())\n        rouge_scores.append(rouge_score)\n        bleu_scores.append(bleu_score)\n\n    average_bleu = sum(bleu_scores) / total\n    average_rouge = sum(rouge_scores) / total\n    print(f\"GSM8K Average BLEU Score: {average_bleu:.4f}\")\n    print(f\"GSM8K Average ROUGE-L Score: {average_rouge:.4f}\")\n    return accuracy, average_bleu, average_rouge\n\n\ndef extract_answer_number(text):\n    \"\"\"Extract the last number from the model's text output.\"\"\"\n    matches = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)  # Find all integers or decimals\n    if matches:\n        print(int(round(float(matches[-1]))))\n        return int(round(float(matches[-1])))# Return the last number as an integer\n    return None\n\nif __name__ == \"__main__\":\n    # Run evaluations\n    mmlu_m_acc,mmlu_m_bleu,mmlu_m_rouge = evaluate_mmlu(\"college_mathematics\")\n    mmlu_cs_acc,mmlu_cs_bleu,mmlu_cs_rouge = evaluate_mmlu(\"college_computer_science\")\n    # gsm_bleu,gsm_rouge = evaluate_gsm8k()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz8BNhDANYWG","outputId":"7804de04-8676-4ab6-bcc3-04277a4d2b1e","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T13:29:11.917550Z","iopub.execute_input":"2024-12-19T13:29:11.917920Z","iopub.status.idle":"2024-12-19T15:51:52.544488Z","shell.execute_reply.started":"2024-12-19T13:29:11.917886Z","shell.execute_reply":"2024-12-19T15:51:52.543510Z"}},"outputs":[{"name":"stdout","text":"Evaluating on MMLU...\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing MMLU:   1%|          | 1/100 [00:52<1:25:55, 52.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   2%|▏         | 2/100 [01:25<1:07:36, 41.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   3%|▎         | 3/100 [02:37<1:29:09, 55.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   4%|▍         | 4/100 [03:00<1:07:51, 42.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   5%|▌         | 5/100 [04:05<1:19:58, 50.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   6%|▌         | 6/100 [05:10<1:26:54, 55.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   7%|▋         | 7/100 [09:48<3:18:57, 128.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   8%|▊         | 8/100 [11:03<2:50:41, 111.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   9%|▉         | 9/100 [11:15<2:01:36, 80.18s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  10%|█         | 10/100 [11:37<1:33:34, 62.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  11%|█         | 11/100 [12:08<1:18:18, 52.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  12%|█▏        | 12/100 [13:17<1:24:34, 57.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  13%|█▎        | 13/100 [13:48<1:11:58, 49.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"5\nanswer:0 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  14%|█▍        | 14/100 [14:55<1:18:24, 54.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:1 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  15%|█▌        | 15/100 [16:27<1:33:29, 66.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  16%|█▌        | 16/100 [17:07<1:21:19, 58.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  17%|█▋        | 17/100 [17:25<1:03:47, 46.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  18%|█▊        | 18/100 [19:13<1:28:35, 64.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  19%|█▉        | 19/100 [19:59<1:19:45, 59.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  20%|██        | 20/100 [20:20<1:03:19, 47.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"5\nanswer:0 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  21%|██        | 21/100 [21:31<1:11:56, 54.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"15\nanswer:3 generated:15\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  22%|██▏       | 22/100 [22:07<1:03:48, 49.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"5\nanswer:3 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  23%|██▎       | 23/100 [22:37<55:42, 43.41s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  24%|██▍       | 24/100 [23:02<47:47, 37.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  25%|██▌       | 25/100 [23:32<44:18, 35.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  26%|██▌       | 26/100 [24:36<54:15, 44.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  27%|██▋       | 27/100 [27:10<1:33:51, 77.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  28%|██▊       | 28/100 [28:06<1:25:02, 70.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  29%|██▉       | 29/100 [28:35<1:08:49, 58.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  30%|███       | 30/100 [29:01<56:39, 48.56s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  31%|███       | 31/100 [29:36<51:17, 44.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  32%|███▏      | 32/100 [34:05<2:06:41, 111.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"135\nanswer:1 generated:135\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  33%|███▎      | 33/100 [35:21<1:52:56, 101.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  34%|███▍      | 34/100 [35:37<1:23:13, 75.65s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  35%|███▌      | 35/100 [36:23<1:12:19, 66.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  36%|███▌      | 36/100 [36:56<1:00:11, 56.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  37%|███▋      | 37/100 [37:18<48:24, 46.10s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  38%|███▊      | 38/100 [37:45<41:38, 40.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  39%|███▉      | 39/100 [37:58<32:40, 32.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  40%|████      | 40/100 [38:29<31:55, 31.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  41%|████      | 41/100 [38:50<28:05, 28.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  42%|████▏     | 42/100 [39:20<28:08, 29.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  43%|████▎     | 43/100 [39:45<26:18, 27.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  44%|████▍     | 44/100 [40:08<24:38, 26.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  45%|████▌     | 45/100 [41:02<31:54, 34.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  46%|████▌     | 46/100 [41:21<26:54, 29.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  47%|████▋     | 47/100 [42:16<33:01, 37.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  48%|████▊     | 48/100 [42:47<30:56, 35.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  49%|████▉     | 49/100 [43:00<24:33, 28.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  50%|█████     | 50/100 [43:15<20:33, 24.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  51%|█████     | 51/100 [44:10<27:32, 33.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  52%|█████▏    | 52/100 [44:34<24:42, 30.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  53%|█████▎    | 53/100 [45:06<24:26, 31.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  54%|█████▍    | 54/100 [46:05<30:20, 39.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  55%|█████▌    | 55/100 [47:22<37:53, 50.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  56%|█████▌    | 56/100 [48:10<36:39, 49.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  57%|█████▋    | 57/100 [48:37<30:49, 43.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  58%|█████▊    | 58/100 [49:19<29:52, 42.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  59%|█████▉    | 59/100 [50:24<33:49, 49.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  60%|██████    | 60/100 [52:38<49:51, 74.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"9\nanswer:0 generated:9\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  61%|██████    | 61/100 [53:03<38:49, 59.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  62%|██████▏   | 62/100 [53:32<31:59, 50.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  63%|██████▎   | 63/100 [53:45<24:15, 39.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  64%|██████▍   | 64/100 [54:16<22:06, 36.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  65%|██████▌   | 65/100 [54:22<16:02, 27.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  66%|██████▌   | 66/100 [54:48<15:22, 27.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  67%|██████▋   | 67/100 [55:12<14:28, 26.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  68%|██████▊   | 68/100 [55:42<14:35, 27.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  69%|██████▉   | 69/100 [56:52<20:41, 40.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  70%|███████   | 70/100 [57:22<18:33, 37.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  71%|███████   | 71/100 [58:34<22:59, 47.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  72%|███████▏  | 72/100 [59:13<20:59, 44.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  73%|███████▎  | 73/100 [1:00:05<21:15, 47.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  74%|███████▍  | 74/100 [1:00:37<18:28, 42.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"-3\nanswer:3 generated:-3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  75%|███████▌  | 75/100 [1:01:47<21:09, 50.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  76%|███████▌  | 76/100 [1:02:34<19:49, 49.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"52\nanswer:3 generated:52\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  77%|███████▋  | 77/100 [1:02:54<15:38, 40.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  78%|███████▊  | 78/100 [1:03:20<13:19, 36.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:2 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  79%|███████▉  | 79/100 [1:04:23<15:32, 44.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  80%|████████  | 80/100 [1:04:40<12:02, 36.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  81%|████████  | 81/100 [1:09:04<33:01, 104.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  82%|████████▏ | 82/100 [1:09:53<26:19, 87.76s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  83%|████████▎ | 83/100 [1:10:49<22:11, 78.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  84%|████████▍ | 84/100 [1:11:26<17:33, 65.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"35\nanswer:2 generated:35\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  85%|████████▌ | 85/100 [1:12:17<15:24, 61.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  86%|████████▌ | 86/100 [1:12:47<12:07, 51.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  87%|████████▋ | 87/100 [1:13:33<10:51, 50.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  88%|████████▊ | 88/100 [1:14:31<10:30, 52.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  89%|████████▉ | 89/100 [1:15:08<08:47, 47.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  90%|█████████ | 90/100 [1:15:31<06:44, 40.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  91%|█████████ | 91/100 [1:16:33<07:02, 46.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  92%|█████████▏| 92/100 [1:16:44<04:48, 36.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  93%|█████████▎| 93/100 [1:17:25<04:24, 37.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  94%|█████████▍| 94/100 [1:18:35<04:43, 47.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  95%|█████████▌| 95/100 [1:19:12<03:40, 44.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  96%|█████████▌| 96/100 [1:19:42<02:39, 39.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  97%|█████████▋| 97/100 [1:20:09<01:48, 36.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  98%|█████████▊| 98/100 [1:21:46<01:48, 54.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  99%|█████████▉| 99/100 [1:22:51<00:57, 57.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU: 100%|██████████| 100/100 [1:24:24<00:00, 50.64s/it]","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\nMMLU Accuracy: 0.3700 (37/100)\nMMLU Average BLEU Score: 0.1159\nMMLU Average ROUGE-L Score: 0.0137\nEvaluating on MMLU...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e917ee48e334ae8b83e9c3584f4abd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/6.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734e577a4b7346dbb34462dd2cc726e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/6.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c405032a8d1247f983557a40ebb0800f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6aec4240ab448e887c7e5beb22be153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c7fdb4adc94004b9de1e14f2c3345f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0430f6df07cd4531ae4d21f8458a9d57"}},"metadata":{}},{"name":"stderr","text":"Processing MMLU:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nProcessing MMLU:   1%|          | 1/100 [00:19<32:10, 19.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   2%|▏         | 2/100 [00:44<37:14, 22.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   3%|▎         | 3/100 [01:44<1:03:57, 39.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   4%|▍         | 4/100 [02:32<1:08:42, 42.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   5%|▌         | 5/100 [03:15<1:08:11, 43.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   6%|▌         | 6/100 [03:47<1:01:48, 39.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   7%|▋         | 7/100 [04:00<47:39, 30.74s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   8%|▊         | 8/100 [04:14<38:57, 25.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   9%|▉         | 9/100 [04:34<35:44, 23.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  10%|█         | 10/100 [05:05<39:01, 26.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  11%|█         | 11/100 [05:29<37:41, 25.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  12%|█▏        | 12/100 [05:54<36:44, 25.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  13%|█▎        | 13/100 [07:57<1:19:32, 54.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  14%|█▍        | 14/100 [08:24<1:06:31, 46.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  15%|█▌        | 15/100 [09:07<1:04:17, 45.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  16%|█▌        | 16/100 [09:44<1:00:03, 42.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  17%|█▋        | 17/100 [10:10<52:24, 37.89s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  18%|█▊        | 18/100 [11:21<1:05:21, 47.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  19%|█▉        | 19/100 [11:36<51:03, 37.83s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  20%|██        | 20/100 [12:02<45:52, 34.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  21%|██        | 21/100 [12:12<35:23, 26.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:3 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  22%|██▏       | 22/100 [12:42<36:12, 27.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  23%|██▎       | 23/100 [12:53<29:29, 22.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  24%|██▍       | 24/100 [13:30<34:19, 27.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  25%|██▌       | 25/100 [13:48<30:38, 24.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  26%|██▌       | 26/100 [14:17<31:51, 25.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  27%|██▋       | 27/100 [14:27<25:23, 20.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:2 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  28%|██▊       | 28/100 [14:47<24:50, 20.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  29%|██▉       | 29/100 [15:18<28:02, 23.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  30%|███       | 30/100 [15:31<24:10, 20.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  31%|███       | 31/100 [16:00<26:32, 23.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  32%|███▏      | 32/100 [16:33<29:31, 26.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  33%|███▎      | 33/100 [16:48<25:33, 22.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"10\nanswer:3 generated:10\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  34%|███▍      | 34/100 [17:10<24:38, 22.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  35%|███▌      | 35/100 [17:36<25:38, 23.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  36%|███▌      | 36/100 [17:48<21:27, 20.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  37%|███▋      | 37/100 [18:02<19:06, 18.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  38%|███▊      | 38/100 [18:56<30:01, 29.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  39%|███▉      | 39/100 [19:28<30:14, 29.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  40%|████      | 40/100 [19:40<24:35, 24.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  41%|████      | 41/100 [19:58<22:17, 22.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  42%|████▏     | 42/100 [20:33<25:25, 26.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  43%|████▎     | 43/100 [21:17<29:57, 31.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  44%|████▍     | 44/100 [21:53<30:38, 32.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"-1\nanswer:3 generated:-1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  45%|████▌     | 45/100 [22:05<24:31, 26.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:3 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  46%|████▌     | 46/100 [22:35<24:54, 27.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  47%|████▋     | 47/100 [23:16<27:55, 31.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  48%|████▊     | 48/100 [23:42<25:54, 29.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  49%|████▉     | 49/100 [24:51<35:23, 41.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2048\nanswer:0 generated:2048\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  50%|█████     | 50/100 [25:22<32:03, 38.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  51%|█████     | 51/100 [25:42<26:46, 32.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  52%|█████▏    | 52/100 [25:54<21:27, 26.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:1 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  53%|█████▎    | 53/100 [26:28<22:42, 28.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  54%|█████▍    | 54/100 [26:50<20:32, 26.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  55%|█████▌    | 55/100 [27:04<17:15, 23.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  56%|█████▌    | 56/100 [27:24<16:11, 22.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"6\nanswer:2 generated:6\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  57%|█████▋    | 57/100 [28:01<18:59, 26.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  58%|█████▊    | 58/100 [28:31<19:11, 27.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  59%|█████▉    | 59/100 [29:18<22:54, 33.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  60%|██████    | 60/100 [29:51<22:15, 33.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  61%|██████    | 61/100 [30:13<19:22, 29.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  62%|██████▏   | 62/100 [30:33<17:03, 26.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  63%|██████▎   | 63/100 [30:55<15:35, 25.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  64%|██████▍   | 64/100 [31:07<12:49, 21.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  65%|██████▌   | 65/100 [31:57<17:29, 29.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  66%|██████▌   | 66/100 [34:10<34:34, 61.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  67%|██████▋   | 67/100 [34:36<27:47, 50.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  68%|██████▊   | 68/100 [35:00<22:39, 42.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  69%|██████▉   | 69/100 [35:38<21:13, 41.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  70%|███████   | 70/100 [35:56<17:03, 34.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1001234\nanswer:0 generated:1001234\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  71%|███████   | 71/100 [36:21<15:09, 31.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  72%|███████▏  | 72/100 [36:48<14:07, 30.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  73%|███████▎  | 73/100 [37:49<17:45, 39.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  74%|███████▍  | 74/100 [38:32<17:33, 40.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  75%|███████▌  | 75/100 [39:00<15:20, 36.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  76%|███████▌  | 76/100 [39:31<13:55, 34.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  77%|███████▋  | 77/100 [40:01<12:50, 33.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  78%|███████▊  | 78/100 [40:46<13:31, 36.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1\nanswer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  79%|███████▉  | 79/100 [40:56<10:06, 28.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:2 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  80%|████████  | 80/100 [41:51<12:12, 36.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"94\nanswer:3 generated:94\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  81%|████████  | 81/100 [42:08<09:47, 30.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  82%|████████▏ | 82/100 [42:29<08:20, 27.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  83%|████████▎ | 83/100 [42:57<07:55, 27.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  84%|████████▍ | 84/100 [43:21<07:05, 26.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  85%|████████▌ | 85/100 [44:09<08:17, 33.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  86%|████████▌ | 86/100 [48:36<24:06, 103.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"10\nanswer:3 generated:10\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  87%|████████▋ | 87/100 [49:09<17:46, 82.06s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  88%|████████▊ | 88/100 [49:45<13:41, 68.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  89%|████████▉ | 89/100 [50:35<11:29, 62.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  90%|█████████ | 90/100 [51:01<08:38, 51.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  91%|█████████ | 91/100 [52:14<08:44, 58.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  92%|█████████▏| 92/100 [52:50<06:52, 51.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  93%|█████████▎| 93/100 [53:29<05:34, 47.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  94%|█████████▍| 94/100 [54:01<04:17, 42.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  95%|█████████▌| 95/100 [55:13<04:17, 51.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\nanswer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  96%|█████████▌| 96/100 [55:33<02:48, 42.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  97%|█████████▋| 97/100 [55:55<01:48, 36.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"4\nanswer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  98%|█████████▊| 98/100 [56:17<01:03, 31.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  99%|█████████▉| 99/100 [57:16<00:40, 40.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"2\nanswer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU: 100%|██████████| 100/100 [58:12<00:00, 34.92s/it]","output_type":"stream"},{"name":"stdout","text":"3\nanswer:3 generated:3\nMMLU Accuracy: 0.3300 (33/100)\nMMLU Average BLEU Score: 0.0653\nMMLU Average ROUGE-L Score: 0.0234\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Example of adding a row to the DataFrame\nevaluation_results.loc[len(evaluation_results)] = [\"Llama-3.1-8B-Finetuned-COT\",\"MMLU\",\"College Mathematics\",mmlu_m_acc,mmlu_m_bleu,mmlu_m_rouge]\n\nevaluation_results.loc[len(evaluation_results)] = [\"Llama-3.1-8B-Finetuned-COT\",\"MMLU\",\"College Computer Science\", mmlu_cs_acc, mmlu_cs_bleu,mmlu_cs_rouge]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:54:17.492189Z","iopub.execute_input":"2024-12-19T15:54:17.492557Z","iopub.status.idle":"2024-12-19T15:54:17.502055Z","shell.execute_reply.started":"2024-12-19T15:54:17.492525Z","shell.execute_reply":"2024-12-19T15:54:17.501080Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"evaluation_results.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:54:29.821055Z","iopub.execute_input":"2024-12-19T15:54:29.821635Z","iopub.status.idle":"2024-12-19T15:54:29.838180Z","shell.execute_reply.started":"2024-12-19T15:54:29.821597Z","shell.execute_reply":"2024-12-19T15:54:29.837479Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"                   Model Name Dataset                      Task  Accuracy  \\\n0  Llama-3.1-8B-Finetuned-COT    MMLU       College Mathematics      0.37   \n1  Llama-3.1-8B-Finetuned-COT    MMLU  College Computer Science      0.33   \n\n   BLEU Score  ROUGE-L Score  \n0    0.115865       0.013732  \n1    0.065258       0.023390  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Name</th>\n      <th>Dataset</th>\n      <th>Task</th>\n      <th>Accuracy</th>\n      <th>BLEU Score</th>\n      <th>ROUGE-L Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Llama-3.1-8B-Finetuned-COT</td>\n      <td>MMLU</td>\n      <td>College Mathematics</td>\n      <td>0.37</td>\n      <td>0.115865</td>\n      <td>0.013732</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Llama-3.1-8B-Finetuned-COT</td>\n      <td>MMLU</td>\n      <td>College Computer Science</td>\n      <td>0.33</td>\n      <td>0.065258</td>\n      <td>0.023390</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"markdown","source":"dataset = load_dataset(\"cais/mmlu\", 'college_mathematics', split=\"test\")\ndataset[4]","metadata":{"execution":{"iopub.status.busy":"2024-12-19T13:13:44.224401Z","iopub.execute_input":"2024-12-19T13:13:44.225204Z","iopub.status.idle":"2024-12-19T13:13:46.263846Z","shell.execute_reply.started":"2024-12-19T13:13:44.225166Z","shell.execute_reply":"2024-12-19T13:13:46.262991Z"}}},{"cell_type":"markdown","source":"question = dataset[4]['question']\nchoices = dataset[4]['choices']\ntemp = '\\n'\nfor n,c in enumerate(choices):\n    temp += f'{n}) {c}\\n'\ntext = f\"Question: {question}\\nChoices: {temp}\"\n\nres = generate_output(text)\nprint(res)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ff2xwaz_n1h","outputId":"580e7b3c-9c62-4041-80bd-19fee9c8855a","execution":{"iopub.status.busy":"2024-12-19T13:25:38.855071Z","iopub.execute_input":"2024-12-19T13:25:38.855801Z","iopub.status.idle":"2024-12-19T13:26:55.535329Z","shell.execute_reply.started":"2024-12-19T13:25:38.855761Z","shell.execute_reply":"2024-12-19T13:26:55.534444Z"}}},{"cell_type":"code","source":"def base_model_output(model, text):\n  final_input = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n    Role: You are a highly intelligent AI assistant specializing in mathematics and coding. Your expertise includes solving complex math problems, writing and debugging code, explaining mathematical concepts, and providing optimized solutions for coding challenges. When presented with a question or problem, you will: 1. Analyze the problem carefully. 2. Provide clear and concise explanations for your reasoning. 3. Offer step-by-step solutions for math and coding problems. 4. Generate clean, efficient, and well-commented code for programming tasks. You are expected to be accurate, logical, and detailed in your responses.\n    Instruction:\n    - Final output needs to be an answer for the question.\n    - The last sentence needs to be the correct option for the question.\n    - Provide the index of the correct option\n    - Always provide the correct option number at the end\n    - Follow the strictly the Structure of output:\n        Explanation : Elaborate on steps\n        Answer : Correct Answer\n        Option : Correct Option number for the correct answer in the choices\n    - Do not deviate from the format mentioned above\n    - Option can only be any one value in 0,1,2,3 and should only be the option number\n    - Do not hallucinate\n    - Do not deviate from the instructions\n    Example:\n      Question : What is 1 + 2 ?\n      Choices:\n      0) 3\n      1) 1\n      2) 2\n      3) 4\n      Explanation : 1 + 2 adds to 3\n      Answer : the answer is 3\n      Option : 0\n    <|end_header_id|>\n    <|start_header_id|>user<|end_header_id|>\"{text}\"<|eot_id|>\"\"\"\n\n  return extract_final_answer(generate_response(model, tokenizer, final_input))","metadata":{"id":"41-AL4nzZkrQ","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T15:59:17.027644Z","iopub.execute_input":"2024-12-19T15:59:17.027987Z","iopub.status.idle":"2024-12-19T15:59:17.033236Z","shell.execute_reply.started":"2024-12-19T15:59:17.027957Z","shell.execute_reply":"2024-12-19T15:59:17.032356Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n\ndef evaluate_mmlu(name):\n    print(\"Evaluating on MMLU...\")\n\n    # Load MMLU dataset (example subset: 'high_school_mathematics')\n    dataset = load_dataset(\"cais/mmlu\", name, split=\"test\")\n\n    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    bleu_scores = []\n    rouge_scores = []\n    correct = 0\n    total = 0\n\n    for sample in tqdm(dataset, desc=\"Processing MMLU\", total=len(dataset)):\n        question = sample['question']\n        choices = sample['choices']  # List of options (A, B, C, D)\n        answer = sample['answer']  # Correct choice index\n        ground_truth = choices[answer]\n\n        # Create the input to the LLM (formatted question)\n        temp = '\\n'\n        for n,c in enumerate(choices):\n          temp += f'{n}) {c}\\n'\n        input_text = f\"Question: {question}\\nChoices: {temp}\"\n\n        # Generate response\n        output = base_model_output(model_finetuned,input_text).strip()\n        number = extract_answer_number(output)\n        rouge_score = rouge.score(ground_truth, output)['rougeL'].fmeasure\n        bleu_score = sentence_bleu([ground_truth.split()], output.split())\n        for i in range(len(output),len(ground_truth)):\n          if i + len(ground_truth) <= len(output):\n            rouge_score = max(rouge_score,rouge.score(ground_truth, output[i:i + len(ground_truth)])['rougeL'].fmeasure)\n            bleu_score = max(rouge_score,sentence_bleu([ground_truth.split()], output[i:i + len(ground_truth)].split()))\n        rouge_scores.append(rouge_score)\n        bleu_scores.append(bleu_score)\n\n        # Match output with options\n        print(f'answer:{answer} generated:{number}')\n        if answer == number:\n          correct += 1\n\n        total += 1\n\n    accuracy = correct / total\n    average_bleu = sum(bleu_scores) / total\n    average_rouge = sum(rouge_scores) / total\n    print(f\"MMLU Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    print(f\"MMLU Average BLEU Score: {average_bleu:.4f}\")\n    print(f\"MMLU Average ROUGE-L Score: {average_rouge:.4f}\")\n    return accuracy, average_bleu, average_rouge\n\ndef evaluate_gsm8k():\n    print(\"Evaluating on GSM8K...\")\n\n    # Load GSM8K dataset\n    dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n\n    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    bleu_scores = []\n    rouge_scores = []\n    correct = 0\n    total = 0\n\n    for sample in tqdm(dataset, desc=\"Processing GSM8K\", total=len(dataset)):\n        question = sample['question']\n        ground_truth = sample['answer']  # Ground truth answer as string\n\n        # Generate response\n        input_text = f\"Question: {question}\\nAnswer:\"\n        output = base_model_output(model_finetuned,input_text).strip()\n\n        # Calculate ROUGE and BLEU\n        rouge_score = rouge.score(ground_truth, output)['rougeL'].fmeasure\n        bleu_score = sentence_bleu([ground_truth.split()], output.split())\n        rouge_scores.append(rouge_score)\n        bleu_scores.append(bleu_score)\n\n    average_bleu = sum(bleu_scores) / total\n    average_rouge = sum(rouge_scores) / total\n    print(f\"GSM8K Average BLEU Score: {average_bleu:.4f}\")\n    print(f\"GSM8K Average ROUGE-L Score: {average_rouge:.4f}\")\n    return accuracy, average_bleu, average_rouge\n\ndef extract_answer_number(text):\n    \"\"\"Extract the last number from the model's text output.\"\"\"\n    matches = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)  # Find all integers or decimals\n    if matches:\n        return int(round(float(matches[-1])))  # Return the last number as an integer\n    return None\n\nif __name__ == \"__main__\":\n    # Run evaluations\n    # mmlu_m_acc2,mmlu_m_bleu2,mmlu_m_rouge2 = evaluate_mmlu(\"college_mathematics\")\n    mmlu_cs_acc2,mmlu_cs_bleu2,mmlu_cs_rouge2 = evaluate_mmlu(\"college_computer_science\")\n    # gsm_bleu2,gsm_rouge2 = evaluate_gsm8k()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TmkNcE3jBB3","outputId":"5ce0961f-6a28-4eaf-a88f-0ba6cad114d9","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T16:23:04.664791Z","iopub.execute_input":"2024-12-19T16:23:04.665169Z"}},"outputs":[{"name":"stdout","text":"Evaluating on MMLU...\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   1%|          | 1/100 [00:54<1:30:21, 54.76s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   2%|▏         | 2/100 [01:16<57:46, 35.37s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   3%|▎         | 3/100 [01:51<56:50, 35.16s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   4%|▍         | 4/100 [02:52<1:12:46, 45.48s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   5%|▌         | 5/100 [03:28<1:06:22, 41.92s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:23\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   6%|▌         | 6/100 [04:01<1:00:55, 38.89s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   7%|▋         | 7/100 [04:38<59:23, 38.32s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:13\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   8%|▊         | 8/100 [06:06<1:23:08, 54.22s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   9%|▉         | 9/100 [06:54<1:19:08, 52.19s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  10%|█         | 10/100 [07:11<1:02:09, 41.44s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  11%|█         | 11/100 [07:28<50:12, 33.85s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  12%|█▏        | 12/100 [08:10<53:05, 36.20s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  13%|█▎        | 13/100 [08:43<51:16, 35.36s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  14%|█▍        | 14/100 [08:59<42:21, 29.56s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  15%|█▌        | 15/100 [10:46<1:15:03, 52.98s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  16%|█▌        | 16/100 [11:08<1:01:07, 43.66s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  17%|█▋        | 17/100 [11:58<1:03:00, 45.54s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  18%|█▊        | 18/100 [12:46<1:03:03, 46.14s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  19%|█▉        | 19/100 [13:08<52:41, 39.03s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  20%|██        | 20/100 [13:12<38:02, 28.53s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  21%|██        | 21/100 [13:37<35:54, 27.28s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  22%|██▏       | 22/100 [14:30<45:42, 35.15s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  23%|██▎       | 23/100 [14:55<41:08, 32.06s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  24%|██▍       | 24/100 [15:23<39:00, 30.79s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:9\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  25%|██▌       | 25/100 [15:40<33:17, 26.64s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  26%|██▌       | 26/100 [16:33<42:29, 34.46s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  27%|██▋       | 27/100 [20:50<2:03:12, 101.26s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  28%|██▊       | 28/100 [21:10<1:32:17, 76.91s/it] ","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  29%|██▉       | 29/100 [22:55<1:41:01, 85.38s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  30%|███       | 30/100 [22:59<1:11:08, 60.98s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  31%|███       | 31/100 [23:26<58:21, 50.75s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  32%|███▏      | 32/100 [24:38<1:04:56, 57.30s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  33%|███▎      | 33/100 [24:43<46:18, 41.46s/it]  ","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  34%|███▍      | 34/100 [25:05<39:11, 35.63s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  35%|███▌      | 35/100 [25:58<44:20, 40.93s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  36%|███▌      | 36/100 [27:04<51:40, 48.45s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  37%|███▋      | 37/100 [27:36<45:42, 43.53s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  38%|███▊      | 38/100 [28:30<47:58, 46.43s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  39%|███▉      | 39/100 [29:24<49:33, 48.75s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  40%|████      | 40/100 [29:28<35:20, 35.33s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  41%|████      | 41/100 [29:43<28:41, 29.17s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  42%|████▏     | 42/100 [29:54<23:06, 23.91s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  43%|████▎     | 43/100 [30:38<28:18, 29.80s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  44%|████▍     | 44/100 [31:18<30:48, 33.01s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  45%|████▌     | 45/100 [31:30<24:33, 26.79s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  46%|████▌     | 46/100 [31:41<19:42, 21.90s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  47%|████▋     | 47/100 [32:16<22:56, 25.96s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  48%|████▊     | 48/100 [33:02<27:28, 31.70s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  49%|████▉     | 49/100 [33:42<29:11, 34.34s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  50%|█████     | 50/100 [33:47<21:09, 25.40s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  51%|█████     | 51/100 [34:09<19:55, 24.41s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  52%|█████▏    | 52/100 [34:39<20:53, 26.12s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  53%|█████▎    | 53/100 [35:18<23:32, 30.06s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  54%|█████▍    | 54/100 [35:37<20:34, 26.84s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  55%|█████▌    | 55/100 [36:37<27:27, 36.61s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  56%|█████▌    | 56/100 [36:48<21:21, 29.12s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  57%|█████▋    | 57/100 [37:16<20:32, 28.66s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:5\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  58%|█████▊    | 58/100 [37:48<20:46, 29.68s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  59%|█████▉    | 59/100 [38:50<26:47, 39.22s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  60%|██████    | 60/100 [39:28<25:54, 38.86s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:6\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  61%|██████    | 61/100 [39:42<20:26, 31.45s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  62%|██████▏   | 62/100 [40:02<17:47, 28.09s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  63%|██████▎   | 63/100 [40:53<21:33, 34.95s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  64%|██████▍   | 64/100 [41:40<23:11, 38.64s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  65%|██████▌   | 65/100 [41:48<17:12, 29.49s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  66%|██████▌   | 66/100 [42:38<20:09, 35.56s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  67%|██████▋   | 67/100 [42:42<14:21, 26.11s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  68%|██████▊   | 68/100 [44:23<25:56, 48.64s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  69%|██████▉   | 69/100 [45:56<31:56, 61.81s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  70%|███████   | 70/100 [46:16<24:38, 49.28s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  71%|███████   | 71/100 [50:31<53:41, 111.08s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  72%|███████▏  | 72/100 [52:06<49:30, 106.08s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  73%|███████▎  | 73/100 [52:40<38:07, 84.74s/it] ","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  74%|███████▍  | 74/100 [53:49<34:37, 79.89s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  75%|███████▌  | 75/100 [54:39<29:33, 70.92s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  76%|███████▌  | 76/100 [54:54<21:41, 54.21s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  77%|███████▋  | 77/100 [55:06<15:54, 41.52s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  78%|███████▊  | 78/100 [56:34<20:17, 55.33s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  79%|███████▉  | 79/100 [57:03<16:38, 47.57s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  80%|████████  | 80/100 [57:17<12:29, 37.47s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  81%|████████  | 81/100 [57:40<10:26, 32.96s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  82%|████████▏ | 82/100 [58:31<11:30, 38.38s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  83%|████████▎ | 83/100 [1:00:35<18:12, 64.27s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  84%|████████▍ | 84/100 [1:00:39<12:19, 46.20s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  85%|████████▌ | 85/100 [1:01:43<12:53, 51.56s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  86%|████████▌ | 86/100 [1:01:52<09:03, 38.81s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  87%|████████▋ | 87/100 [1:02:35<08:39, 39.95s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  88%|████████▊ | 88/100 [1:02:39<05:50, 29.17s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  89%|████████▉ | 89/100 [1:02:50<04:19, 23.57s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  90%|█████████ | 90/100 [1:02:54<02:57, 17.71s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  91%|█████████ | 91/100 [1:04:10<05:18, 35.43s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  92%|█████████▏| 92/100 [1:05:10<05:40, 42.62s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  93%|█████████▎| 93/100 [1:09:24<12:23, 106.24s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  94%|█████████▍| 94/100 [1:10:52<10:03, 100.61s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  95%|█████████▌| 95/100 [1:10:56<05:58, 71.63s/it] ","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  96%|█████████▌| 96/100 [1:11:18<03:47, 56.89s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  97%|█████████▋| 97/100 [1:11:57<02:34, 51.54s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  98%|█████████▊| 98/100 [1:13:14<01:58, 59.04s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  99%|█████████▉| 99/100 [1:14:11<00:58, 58.34s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:35\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU: 100%|██████████| 100/100 [1:15:41<00:00, 45.42s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:2\nMMLU Accuracy: 0.2600 (26/100)\nMMLU Average BLEU Score: 0.1298\nMMLU Average ROUGE-L Score: 0.0313\nEvaluating on MMLU...\n","output_type":"stream"},{"name":"stderr","text":"\nProcessing MMLU:   1%|          | 1/100 [00:11<18:15, 11.06s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   2%|▏         | 2/100 [00:58<53:25, 32.71s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   3%|▎         | 3/100 [01:22<45:53, 28.38s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   4%|▍         | 4/100 [01:27<30:57, 19.35s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   5%|▌         | 5/100 [02:17<47:54, 30.26s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   6%|▌         | 6/100 [03:00<54:24, 34.73s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   7%|▋         | 7/100 [03:18<45:12, 29.17s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   8%|▊         | 8/100 [03:32<37:11, 24.26s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:   9%|▉         | 9/100 [03:56<36:55, 24.34s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  10%|█         | 10/100 [04:01<27:34, 18.38s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  11%|█         | 11/100 [04:06<20:58, 14.14s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  12%|█▏        | 12/100 [04:24<22:45, 15.52s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  13%|█▎        | 13/100 [04:29<17:53, 12.34s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  14%|█▍        | 14/100 [04:40<16:54, 11.80s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  15%|█▌        | 15/100 [05:03<21:20, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  16%|█▌        | 16/100 [05:51<35:08, 25.10s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  17%|█▋        | 17/100 [05:55<25:58, 18.78s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  18%|█▊        | 18/100 [05:59<19:40, 14.40s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  19%|█▉        | 19/100 [06:03<15:14, 11.29s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  20%|██        | 20/100 [06:08<12:20,  9.25s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  21%|██        | 21/100 [06:21<13:33, 10.29s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:3\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  22%|██▏       | 22/100 [06:35<15:05, 11.61s/it]","output_type":"stream"},{"name":"stdout","text":"answer:1 generated:1\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  23%|██▎       | 23/100 [06:39<11:59,  9.34s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  24%|██▍       | 24/100 [07:15<21:44, 17.16s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  25%|██▌       | 25/100 [07:43<25:35, 20.47s/it]","output_type":"stream"},{"name":"stdout","text":"answer:0 generated:4\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  26%|██▌       | 26/100 [08:01<24:13, 19.65s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:2\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  27%|██▋       | 27/100 [08:05<18:18, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"answer:2 generated:None\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  28%|██▊       | 28/100 [08:22<18:42, 15.59s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:0\n","output_type":"stream"},{"name":"stderr","text":"Processing MMLU:  29%|██▉       | 29/100 [08:39<19:05, 16.13s/it]","output_type":"stream"},{"name":"stdout","text":"answer:3 generated:None\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Example of adding a row to the DataFrame\nevaluation_results.loc[len(evaluation_results)] = [\"Llama-3.1-8B-Finetuned\",\"MMLU\",\"College Mathematics\",mmlu_m_acc2,mmlu_m_bleu2,mmlu_m_rouge2]\n\nevaluation_results.loc[len(evaluation_results)] = [\"Llama-3.1-8B-Finetuned\",\"MMLU\",\"College Computer Science\", mmlu_cs_acc2, mmlu_cs_bleu2,mmlu_cs_rouge2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluation_results.to_csv('evaluations.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from evaluation (base and fine-tuned)\nmetrics = ['Accuracy', 'BLEU', 'ROUGE']\ncollege_math_base = [0.26, 0.1298, 0.0313]\ncollege_math_finetuned = [0.37, 0.115865, 0.013732]\n\n# Create bar chart\nx = np.arange(len(metrics))  # Positions for the metrics\nwidth = 0.35  # Bar width\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n# College Mathematics\nax[0].bar(x - width/2, college_math_base, width, label='Base Model', color='skyblue')\nax[0].bar(x + width/2, college_math_finetuned, width, label='Fine-tuned Model', color='orange')\nax[0].set_title('College Mathematics')\nax[0].set_xticks(x)\nax[0].set_xticklabels(metrics)\nax[0].set_ylabel('Scores')\nax[0].legend()\n\n# College Computer Science\nax[1].bar(x - width/2, college_cs_base, width, label='Base Model', color='skyblue')\nax[1].bar(x + width/2, college_cs_finetuned, width, label='Fine-tuned Model', color='orange')\nax[1].set_title('College Computer Science')\nax[1].set_xticks(x)\nax[1].set_xticklabels(metrics)\nax[1].set_ylabel('Scores')\nax[1].legend()\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"dC9E-loPvmBa","outputId":"76c904fb-b5ca-4965-977d-864bd65edb1f","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T16:20:31.325493Z","iopub.status.idle":"2024-12-19T16:20:31.325836Z","shell.execute_reply.started":"2024-12-19T16:20:31.325689Z","shell.execute_reply":"2024-12-19T16:20:31.325705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig.savefig('comparision1')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}